## nginx平滑升级
平滑升级时不重启master进程而启动新版本的Nginx程序。旧版本的master进程通过fork创建新进程然后调用execve系统调用来启动新版本程序。这时旧版本的master进程必须通过一种方法告诉新版本的master进程这是在平滑升级，且传递一些必要信息。Nginx在execve时通过环境变量来传递需要打开的监听端口。新版本的Nginx进程监听已经打开的TCP监听端口。

1、master进程收到USR2信号，信号处理函数设置相应标志位。master进程fork出子进程，执行execve操作。其中旧的nginx程序监听的端口以环境变量的形式传递给新进程。 
2、向旧的master进程发送QUIT信号，旧的进程开始执行优雅关闭，使用kill函数向所有的子进程发送QUIT信号，同时关闭所有监听句柄。
3、worker进程收到QUIT信号，将对应标志置1，关闭所有监听句柄。对所有连接调用关闭连接的处理方法，如果还有未处理完事件继续进行事件分发处理，否则调用各个模块的exit_process方法，销毁内存池。
4、子进程退出后，老的master进程作为父进程会收到SIGCHLD信号，master获取所有子进程状态。如果worker进程异常退出，重新拉起新worker进程。当所有worker进程收到了quit信号正常退出时，关闭master进程。

## 事件处理模型
请求的多阶段异步处理，尽量少的出现进程休眠。
在传统模型中，多进程模型，每个进程负责一个连接，每个进程在处理请求过程中长时间阻塞。为保证处理的并发数，所需进程数目多。进程创建开销，切换频繁也增加开销。同时阻塞进程占用内存无法释放，导致可处理的并发数下降。
epoll分发事件，事件的消费处理者只处理1个请求的某个阶段。异步的被动等待epoll下一次事件通知。极大提高了网络性能，使每个进程/线程权力运转，尽可能少的出现进程/线程休眠。
将阻塞进程的方法按照相关的触发事件分解为两个阶段/按照时间分解为多个阶段的方法调用
必须有阻塞方法时，拉起一个临时进程完成。
## slab内存池
放在共享内存中。
1、共享内存起始地址处存放slab共享内存结构体(free空闲页的页描述结构体链表/start指向连续页面的起始地址/end指向连续页面的终止地址/pages指向存放页描述结构体区间起始地址)。
2、半满页链表头节点数组slots(指向同样大小内存块的半满页的页描述结构体组成的链表，8，16，32，64……字节) 如slots[2]指向存放32字节内存块的半满页的页描述链表。
3、页描述结构体(连续存放的pages个页描述结构体，prev/next字段组织链表，slab字段作为bitmap标识页面上内存块的使用情况)
4、pages个连续页面(与页描述结构体一一对应)
分配内存大于1页时，计算所需页面数n。从free中寻找连续的n个页面
当分配字节为单位的内存时，计算所需内存块大小。取得半满页链表数组slots，寻址到内存块对应半满页链表。遍历半满页链表，根据节点页描述结构的slab字段作为bitmap查询内存块使用情况。找到第一个空闲内存块。如果找不到将这个全满页从链表摘除，继续向后遍历。找到后，将空闲内存块对应的bitmap位置1。如果没有可用半满页，从free中分配空闲页，将新页内存块的值设置成当前查询内存块大小，分配内存块，将页面插入到半满页链表头部。

## 负载均衡锁
master进程监听web端口，fork出多个worker进程同时监听相同socket，为防止惊群效应(linux的SO_REUSEPORT:可以启动多个服务器进程，共享一个监听socket。Linux内核会做简单的负载均衡，选择一个进程由accept返回。)使用accept_mutex进程间同步锁。进程非阻塞的获取锁，只有获取到锁的进程才可以监听端口。为防止一个进程长时间的占有accept_mutex，进程将epoll返回事件放入队列，新的连接事件和普通io事件放入不同队列，处理完连接事件后，立刻释放accept_mutex在处理普通的io事件。
当一个进程的连接达到总连接数的7/8时，触发负载均衡，就不再争抢锁接受新的连接。

向上游服务器转发请求时，尽量把请求平均的分到每一台上游服务器上。
轮询（默认）
weight 指定轮询几率，weight和访问比率成正比
在负载均衡系统中，假如用户在某台服务器上登录了，那么该用户第二次请求的时候，因为我们是负载均衡系统，每次请求都会重新定位到服务器集群中的某一个，那么已经登录某一个服务器的用户再重新定位到另一个服务器，其登录信息将会丢失，这样显然是不妥的。
会希望一个用户的请求始终落到一台上游服务器中。使用ip_hash,对客户端的ip地址计算出一个key，及那个key按照upstream集群里的上游服务器数量进行取模，按取模后的索引值把请求发到对应上游服务器。
fair（第三方）：按后端服务器的响应时间来分配请求，响应时间短的优先分配。
url_hash（第三方）：按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。

## 连接重用
某个连接重用时，为了防止串话，使用了instance标志。每建立一次连接。就反转一次instance，表明连接被重用了。如果epoll返回的epoll_event中记录的instance和连接的instance不一致，说明连接被重用了，事件失效了。
instance只有一位(bool)，epoll_event data域存储了指向connection结构体的指针，地址最后一位必为0(字节对齐)。将instance存到了connection地址的最后一位。

## 文件AIO
主机磁盘事件目前不支持epoll，使用内核AIO。
如果页此时不在内核，则对磁盘块的read()或sendfile()将被阻塞；将磁盘文件句柄上设置非阻塞模式没有任何效果。内存映射磁盘文件也是如此。服务器第一次需要磁盘I/O时，它的线程会阻塞，导致所有客户都必须等待。
内核异步I/O，只有当内核完成了磁盘操作才会通知进程。
把读取文件的操作异步提交给内核后，内核通知I/O设备独立的执行操作，进程可以继续充分占用cpu。大量读事件堆积到I/O设备队列时，将发挥内核中电梯算法的优势，降低随机读取磁盘的成本(电梯总是保持一个方向运行，直到该方向没有请求为止，然后改变运行方向。电梯算法（扫描算法）和电梯的运行过程类似，总是按一个方向来进行磁盘调度，直到该方向上没有未完成的磁盘请求，然后改变方向。)。内核级别文件AIO不支持缓存。即使操作文件块在linux文件缓存中存在，也不会读取、更改缓存中的文件来代替实际对磁盘的操作。虽然从阻塞worker进程角度将使用AIO有好处，但是对于单个请求，可能降低了处理速度(原先可以从内存中快速获取的文件块使用AIO后会从磁盘中读取)。如果大部分用户请求对文件操作会落到文件缓存中，就不必使用AIO，否则使用AIO可以提高并发能力。正常写入文件时往往写入内存就返回，效率远高于AIO写入磁盘。nginx仅支持在读取大于512k文件时使用AIO，避免大文件挤占文件系统缓存。大文件使用AIO减少进程阻塞，小文件使用sendfile，减少拷贝次数。(文件的偏移和大小必须按照磁盘块大小对齐(512B),并且要求DIRECTIO不能使用VM cache）

glibc 异步I/O 线程+阻塞调用在用户空间模拟 AIO 的功能并非真正意义上的异步I/O。
io_setup 初始化Aio上下文 aio_context_t io_submit 提交异步io请求 io_getevents io_cancel io_destroy
struct iocb(data保留字,可以存放AIO完成后的回调方法/操作码opcode/请求对应文件描述符/对应用户态缓冲区/文件偏移量/aio_resfd字段可选择异步I/O完成使用eventfd进行通知)
返回io_event结构体(指向对应iocb结构体)
在eventfd可读从epoll返回后，在回调函数中调用io_getevents获取异步i/o成功后返回的io_event结构体,将data域中存放的回调函数指针放入任务队列中执行。

sendfile(零拷贝，没有内核态到用户态的拷贝)：启用Linux上的sendfile系统调用来发送文件，它减少了内核态与用户态之间内存复制，没有用户态和内核态之间的切换，这样就会从磁盘中读取文件后直接在内核态发送到网卡设备，提高了发送文件的效率。 sendfile() 只是适用于应用程序地址空间不需要对所访问数据进行处理的情况。sendfile系统调用利用DMA引擎将文件数据拷贝到内核缓冲区，之后数据被拷贝到内核socket缓冲区中，DMA引擎将数据从内核socket缓冲区拷贝到协议引擎中。linux2.4以后，当文件数据被复制到内核缓冲区时，不再将所有数据copy到socket相关的缓冲区，而是仅仅将记录数据位置和长度相关的数据保存到socket相关的缓存，而实际数据将由DMA模块直接发送到协议引擎。

sendfile：maxchunk可以减少阻塞调用sendfile()所花费的最长时间.因为Nginx不会尝试一次将整个文件发送出去,而是每次发送大小为256KB的块数据。
1、异步操作，进程不会睡眠，不占用cpu，可以占用cpu 2、应用程序空间可以对所访问的数据进行处理

I/O 控制方式：
1、直接I/O(轮询)
2、中断
3、DMA(直接主存存取方式,不占用CPU)

mmap系统调用是将硬盘文件映射到用内存中，将page cache中的页直接映射到用户进程地址空间中，从而进程可以直接访问自身地址空间的虚拟地址来访问page cache中的页。
访问文件，无论通过标准io还是mmap缺页，第一步都是读入page cache，这时用的是内核的虚拟地址，该地址映射到page cache的物理地址。然后，如果是文件io，就是拷贝到用户空间映射的物理地址上，如果是mmap，该用户地址空间也映射到相同的page cache物理地址。(page cache 是个物理上的概念，使用buffered io情况下,从磁盘物理上拷贝到page cache。如果是mmap，内核空间和用户空间都能映射到这块物理区域。如果是文件io，用户空间映射到用户buffer，内核空间映射到page cache。需要从page cache 物理拷贝到用户buffer)。

但是由于linux本身用的还是页式管理，page cache不可避免的会导致缺页中断的现象，这时候会进行页面置换算法，增加耗时。
对此，linux本身提供了dio的接口，使得数据直接从应用程序地址空间拷贝硬盘上。如果要加快读写，可以使用directio，自己设定缓冲区，而不是用pagecache。



## 同步
原子操作：通过内联汇编代码实现原子操作。使用volatile关键字限制gcc编译器对代码优化。多核架构下通过 lock前缀+cmpxchg命令实现 cas(compare and swap)
自旋锁：使用锁的时间非常短。不希望进程睡眠，特别是核心事件，进程睡眠和唤醒带来线程间切换消耗极大。
文件锁 fcntl

1、linux自旋锁：在单处理器系统中，就是禁止内核抢占。
2、自旋锁基于原子操作实现：在一个循环中。lock为0表示没有人持有锁，加锁时如果lock为0就通过cas操作将lock值原子的从0修改为value，然后返回。否则在循环中等待。多核架构下，不立即让出cpu，而是等待一段时间，看看其他处理器上的进程是否会释放锁，这样会减少进程间的切换次数。循环次数到达一个门限，调用yield()暂时让出cpu，使得cpu优先调度其他可执行状态的进程。当进程被内核再次调度时，在循环中再次等待其他进程释放锁。
关于cas：通过内联汇编代码实现原子操作。使用volatile关键字限制gcc编译器对代码优化。多核架构下通过 lock前缀+cmpxchg命令实现

nginx互斥锁实现：
不支持原子操作：文件锁
支持原子操作：原子操作和自旋锁
支持原子操作和信号量：原子操作+信号量+自旋锁 锁是一个原子变量。
加锁时最高位从0->1,只要锁的值非负，就表示没有进程持有锁。如果锁为负值，自旋锁达到spin上限，将锁的值加1，sem_wait进入睡眠。
解锁时，将最高位1->0,如果锁不为0，说明有进程睡眠等待，sem_post将其唤醒。

linux 读写锁的实现
linux mutex 实现公平锁 加锁排队任务太重。

## 时间
防止太多的gettimeofday系统调用，缓存了时间。在nginx中，启动时调用gettimeofday更新一次，worker进程每次从epoll返回进行事件分发前检查相关标志位调用gettimeofday更新时间值。为防止process_events方法调用频率低，可以设置时间分辨率time_resolution,调用setitimer调用，每隔time_resolution将更新时间的标志位置1，这时只要有进程从epoll返回就会更新时间。为防止进程长时间阻塞在epoll_wait上，将超时时间设置0立马返回。(设置定时任务按照时间分辨率调用gettimeofday更新时间缓存，其他线程只是读这个时间缓存，可以用内核seq锁的思路。一写多读，写的优先级更高。现在gettimeofday优化了，不是每次都系统调用，netty就没有缓存时间)
使用红黑树组织定时器，没有使用堆。堆的内存局部性会更好一些。




